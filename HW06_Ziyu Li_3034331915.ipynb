{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-X Spring 2019: Homework 06 \n",
    "\n",
    "## Name : Ziyu Li\n",
    "## SID : 3034331915\n",
    "## Course (IEOR 135/290) : IEOR 290\n",
    "\n",
    "### Machine Learning\n",
    "\n",
    "\n",
    "\n",
    "In this homework, you will do some exercises with prediction. We will cover these algorithms in class, but this is for you to have some hands on with these in scikit-learn. You can refer - https://github.com/ikhlaqsidhu/data-x/blob/master/05a-tools-predicition-titanic/titanic.ipynb\n",
    "\n",
    "Display all your outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # machine learning libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__ 1. Read __`diabetesdata.csv`__ file into a pandas dataframe. \n",
    "About the data: __\n",
    "\n",
    "1. __TimesPregnant__: Number of times pregnant \n",
    "2. __glucoseLevel__: Plasma glucose concentration a 2 hours in an oral glucose tolerance test \n",
    "3. __BP__: Diastolic blood pressure (mm Hg)  \n",
    "5. __insulin__: 2-Hour serum insulin (mu U/ml) \n",
    "6. __BMI__: Body mass index (weight in kg/(height in m)^2) \n",
    "7. __pedigree__: Diabetes pedigree function \n",
    "8. __Age__: Age (years) \n",
    "9. __IsDiabetic__: 0 if not diabetic or 1 if diabetic) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TimesPregnant  glucoseLevel  BP  insulin   BMI  Pedigree   Age  IsDiabetic\n",
      "0              6         148.0  72        0  33.6     0.627  50.0           1\n",
      "1              1           NaN  66        0  26.6     0.351  31.0           0\n",
      "2              8         183.0  64        0  23.3     0.672   NaN           1\n",
      "3              1           NaN  66       94  28.1     0.167  21.0           0\n",
      "4              0         137.0  40      168  43.1     2.288  33.0           1\n"
     ]
    }
   ],
   "source": [
    "#Read data & print the head\n",
    "df=pd.read_csv('diabetesdata.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Calculate the percentage of Null values in each column and display it. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimesPregnant    0.000000\n",
       "glucoseLevel     4.427083\n",
       "BP               0.000000\n",
       "insulin          0.000000\n",
       "BMI              0.000000\n",
       "Pedigree         0.000000\n",
       "Age              4.296875\n",
       "IsDiabetic       0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Split __`data`__  into  __`train_df`__ and __`test_df`__  with 15% as test.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Display the means of the features in train and test sets. Replace the null values in  __`train_df`__ and __`test_df`__  with the mean of EACH feature column separately for train and test. Display head of the dataframes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimesPregnant      3.991379\n",
       "glucoseLevel     121.135135\n",
       "BP                67.068966\n",
       "insulin           72.715517\n",
       "BMI               31.214655\n",
       "Pedigree           0.485888\n",
       "Age               31.396396\n",
       "IsDiabetic         0.293103\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the means of the features in train and test sets\n",
    "train_df.mean()\n",
    "test_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     TimesPregnant  glucoseLevel  BP  insulin   BMI  Pedigree   Age  \\\n",
      "461              1          71.0  62        0  21.8     0.416  26.0   \n",
      "285              7         136.0  74      135  26.0     0.647  51.0   \n",
      "154              8         188.0  78        0  47.9     0.137  43.0   \n",
      "739              1         102.0  74        0  39.5     0.293  42.0   \n",
      "191              9         123.0  70       94  33.1     0.374  40.0   \n",
      "\n",
      "     IsDiabetic  \n",
      "461           0  \n",
      "285           0  \n",
      "154           1  \n",
      "739           1  \n",
      "191           0  \n",
      "     TimesPregnant  glucoseLevel  BP  insulin   BMI  Pedigree        Age  \\\n",
      "201              1         138.0  82        0  40.1     0.236  28.000000   \n",
      "95               6         144.0  72      228  33.9     0.255  40.000000   \n",
      "497              2          81.0  72       76  30.1     0.547  25.000000   \n",
      "751              1         121.0  78       74  39.0     0.261  28.000000   \n",
      "299              8         112.0  72        0  23.6     0.840  31.396396   \n",
      "\n",
      "     IsDiabetic  \n",
      "201           0  \n",
      "95            0  \n",
      "497           0  \n",
      "751           0  \n",
      "299           0  \n"
     ]
    }
   ],
   "source": [
    "# replace the null value with the mean\n",
    "train_df=train_df.fillna(train_df.mean())\n",
    "test_df=test_df.fillna(test_df.mean())\n",
    "print(train_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Split __`train_df`__ & __`test_df`__   into  __`X_train`__, __`Y_train`__  and __`X_test`__, __`Y_test`__. __`Y_train`__  and __`Y_test`__ should only have the column we are trying to predict,  __`IsDiabetic`__.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop('IsDiabetic', axis=1)\n",
    "Y_train = train_df['IsDiabetic']\n",
    "X_test = test_df.drop('IsDiabetic', axis=1)\n",
    "Y_test = test_df['IsDiabetic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Use this dataset to train perceptron, logistic regression and random forest models using 15% test split. Report training and test accuracies. Try different hyperparameter values for these models and see if you can improve your accuracies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression training accuracy is :  0.7730061349693251\n",
      "Logistic Regression ttest accuracy is :  0.8017241379310345\n"
     ]
    }
   ],
   "source": [
    "# 6a. Logistic Regression\n",
    "logreg = LogisticRegression()                                \n",
    "logreg.fit(X_train, Y_train)                                  \n",
    "log_accuracy_train = logreg.score(X_train, Y_train)  \n",
    "log_accuracy_test = logreg.score(X_test, Y_test)\n",
    "\n",
    "print('Logistic Regression training accuracy is : ', log_accuracy_train)\n",
    "print('Logistic Regression ttest accuracy is : ', log_accuracy_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression training acuracy is  0.7760736196319018\n",
      "logistic regression test accuracy is  0.8103448275862069\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters tuning for logistic regression when c=100,penalty='11'\n",
    "logreg100 = LogisticRegression(penalty='l1',C=100) \n",
    "logreg100.fit(X_train, Y_train)\n",
    "logreg_train_acc = logreg100.score(X_train, Y_train)\n",
    "logreg_test_acc = logreg100.score(X_test, Y_test)\n",
    "print ('logistic regression training acuracy is ',logreg_train_acc)\n",
    "print('logistic regression test accuracy is ',logreg_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression training acuracy is  0.7484662576687117\n",
      "logistic regression test accuracy is  0.8275862068965517\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters tuning for logistic regression when c=0.1,penalty='11'\n",
    "logreg= LogisticRegression(penalty='l1',C=0.1) \n",
    "logreg.fit(X_train, Y_train)\n",
    "logreg_train_acc = logreg.score(X_train, Y_train)\n",
    "logreg_test_acc = logreg.score(X_test, Y_test)\n",
    "print ('logistic regression training acuracy is ',logreg_train_acc)\n",
    "print('logistic regression test accuracy is ',logreg_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron training acuracy is  0.6395705521472392\n",
      "Perceptron test accuracy is  0.6810344827586207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jade/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 6b. Perceptron\n",
    "perceptron = Perceptron()                                     \n",
    "perceptron.fit(X_train, Y_train)                            \n",
    "perceptron_train_acc = perceptron.score(X_train, Y_train) \n",
    "perceptron_test_acc = perceptron.score(X_test, Y_test)\n",
    "print ('Perceptron training acuracy is ',perceptron_train_acc)\n",
    "print('Perceptron test accuracy is ',perceptron_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron training acuracy is  0.3604294478527607\n",
      "Perceptron test accuracy is  0.29310344827586204\n"
     ]
    }
   ],
   "source": [
    "# with hyperparameter penalty='elasticnet', max_iter=50\n",
    "perceptron = Perceptron(penalty='elasticnet',max_iter=50)                                     \n",
    "perceptron.fit(X_train, Y_train)                            \n",
    "perceptron_train_acc = perceptron.score(X_train, Y_train) \n",
    "perceptron_test_acc = perceptron.score(X_test, Y_test)\n",
    "print ('Perceptron training acuracy is ',perceptron_train_acc)\n",
    "print('Perceptron test accuracy is ',perceptron_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest training acuracy is  1.0\n",
      "Random Forest test accuracy is  0.8017241379310345\n"
     ]
    }
   ],
   "source": [
    "# 6c. Random Forest\n",
    "random_forest = RandomForestClassifier(n_estimators=500)   \n",
    "random_forest.fit(X_train, Y_train)                         \n",
    "rf_train_acc = random_forest.score(X_train, Y_train)  \n",
    "rf_test_acc = random_forest.score(X_test, Y_test) \n",
    "print ('Random Forest training acuracy is ',rf_train_acc)\n",
    "print('Random Forest test accuracy is ',rf_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest training acuracy is  0.9187116564417178\n",
      "Random Forest test accuracy is  0.7931034482758621\n"
     ]
    }
   ],
   "source": [
    "# with hyperparameters\n",
    "random_forest = RandomForestClassifier(n_estimators=400,max_features=5,max_depth=70, min_samples_leaf=4,min_samples_split=10)   \n",
    "random_forest.fit(X_train, Y_train)                         \n",
    "rf_train_acc = random_forest.score(X_train, Y_train)  \n",
    "rf_test_acc = random_forest.score(X_test, Y_test) \n",
    "print ('Random Forest training acuracy is ',rf_train_acc)\n",
    "print('Random Forest test accuracy is ',rf_test_acc)                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. For your logistic regression model - **\n",
    "\n",
    "**a . Compute the log probability of classes in  __`IsDiabetic`__ for the first 10 samples of your train set and display it. Also display the predicted class for those samples from your logistic regression model trained before. **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.90665314 0.09334686]\n",
      " [0.56185356 0.43814644]\n",
      " [0.14705185 0.85294815]\n",
      " [0.73241931 0.26758069]\n",
      " [0.48626508 0.51373492]\n",
      " [0.5490879  0.4509121 ]\n",
      " [0.68176572 0.31823428]\n",
      " [0.34175524 0.65824476]\n",
      " [0.82578136 0.17421864]\n",
      " [0.33577044 0.66422956]]\n",
      "[0 0 1 0 1 0 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "X_train_sample=X_train.iloc[:10,:]\n",
    "y_prob=logreg.predict_proba(X_train_sample)\n",
    "print(y_prob)\n",
    "y_pred=logreg.predict(X_train_sample)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b . Now compute the log probability of classes in  __`IsDiabetic`__ for the first 10 samples of your test set and display it. Also display the predicted class for those samples from your logistic regression model trained before.\n",
    " (using the model trained on the training set)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.61198641 0.38801359]\n",
      " [0.44958428 0.55041572]\n",
      " [0.85158002 0.14841998]\n",
      " [0.67838603 0.32161397]\n",
      " [0.6909086  0.3090914 ]\n",
      " [0.44421069 0.55578931]\n",
      " [0.58201399 0.41798601]\n",
      " [0.72799292 0.27200708]\n",
      " [0.78070085 0.21929915]\n",
      " [0.51573869 0.48426131]]\n",
      "[0 1 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "X_test_sample=X_test.iloc[:10,:]\n",
    "y_prob=logreg.predict_proba(X_test_sample)\n",
    "print(y_prob)\n",
    "y_pred=logreg.predict(X_test_sample)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c . What can you interpret from the log probabilities and the predicted classes?**\n",
    "\n",
    "The threshold for this logistic regression model is 0.5, when the log probability of a sample greater than 0.5, it will be classified as class 1 while if smaller than 0.5 it will be classified as class 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Is mean imputation is the best type of imputation (as we did in 4.) to use? Why or why not? What are some other ways to impute the data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean imputation is not the best type of imputation because it will bias the standard error. \n",
    "There are some other ways to impute the data like EM imputation, Hot deck imputation, Cold deck imputation, Regression imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit (2 pts) - MANDATORY for students enrolled in IEOR 290"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**9.  Implement the K-Nearest Neighbours (https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) algorithm for k=1 from scratch in python (do not use KNN from existing libraries). KNN uses Euclidean distance to find nearest neighbors. Split your dataset into test and train as before. Also fill in the null values with mean of features as done earlier. Use this algorithm to predict values for 'IsDiabetic' for your test set. Display your accuracy. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.6896551724138%\n"
     ]
    }
   ],
   "source": [
    "#Define Euclidean distances \n",
    "import math\n",
    "import operator\n",
    "\n",
    "def Euclideandist(x,xi, length): \n",
    "    d = 0.0\n",
    "    for i in range(length):\n",
    "        d += pow(float(x[i])- float(xi[i]),2)\n",
    "    return math.sqrt(d)\n",
    "\n",
    "#Getting neighbours \n",
    "def getNeighbors(trainingSet, testInstance, k):\n",
    "    distances = []\n",
    "    length = len(testInstance)-1\n",
    "    for x in range(len(trainingSet)):\n",
    "        dist = Euclideandist(testInstance, trainingSet[x], length)\n",
    "        distances.append((trainingSet[x], dist)) \n",
    "    distances.sort(key=operator.itemgetter(1)) \n",
    "    neighbors = []\n",
    "    for x in range(k):\n",
    "        neighbors.append(distances[x][0]) \n",
    "    return neighbors\n",
    "\n",
    "# get response\n",
    "def getResponse(neighbors):\n",
    "    classVotes = {}\n",
    "    for x in range(len(neighbors)):\n",
    "        response = neighbors[x][-1] \n",
    "        if response in classVotes:\n",
    "            classVotes[response] += 1 \n",
    "        else:\n",
    "            classVotes[response] = 1\n",
    "    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedVotes[0][0]\n",
    "    \n",
    "# get accuracy\n",
    "def getAccuracy(testSet, predictions): \n",
    "    correct = 0\n",
    "    for x in range(len(testSet)):\n",
    "        if testSet[x][-1] == predictions[x]:\n",
    "            correct += 1\n",
    "    return (correct/float(len(testSet))) * 100.0\n",
    "\n",
    "# generate predictions for k=1 testSet = test_df.values.tolist()\n",
    "testSet=test_df.values.tolist()\n",
    "trainingSet = train_df.values.tolist()\n",
    "\n",
    "predictions=[]\n",
    "k=1\n",
    "for x in range(len(testSet)):\n",
    "    neighbors = getNeighbors(trainingSet, testSet[x], k)\n",
    "    result = getResponse(neighbors)\n",
    "    predictions.append(result)\n",
    "\n",
    "accuracy = getAccuracy(testSet, predictions) \n",
    "print('Accuracy: ' + repr(accuracy) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "data-x"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "0.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
